[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-24ddc0f5d75046c5622901739e7c5dd533143b0c8e959d652212380cedb1ea36.svg)](https://classroom.github.com/a/q_3tyBp7)

# Writeup
The goal of this assesment is to understand various generative models. First question we need to address is how can we compare the samples generated by two methods. We defined a metric based on KL-divergence.  Why loss is not enough? If the training samples are less the loss will be smaller but even the validation loss is not right to compare. Lets give you simple example, Compare loss of Score and diffsion models. $L_{DDPM} = L_{Score} -C_1 + C_2$. where $C_1$
and $C_2$ are constants. 

We cannot used FD ( Frechet Distance ), which is a common metric for Images and it is not applicable in out case  as it assumes normality of the datapoints which is violated for GMMs.

### KL Distance 
$$p_{estimate}(x) = \frac{1}{hN}\Sigma_i^N  K(\frac{x - x_i}{h})n$$
$$q_{true}(x) = \Sigma \alpha_i \mathcal{N}(\mu_i, \sigma_i)$$
$$KLD = KL(q||p)$$
where K is any kernel, here we assume gaussian kernel for simplicity. Here the idea is that we are estimating the probabilty or desinty of the gnerated samples by gaussian kernel and comparing the actual desnity of the x given by q, which is mixture of gaussian. Smaller it is better the generation quality. 

Why are the generative model papers not using this metric. Here we are making huge assumption of estimating likelihood by kernel density. This holds only true for large sample size. For our example as it is a toy data this suits the purpose and we have a large sample size.


### VAE & Hierachial VAE & Learned variance DDPM

### Score based models & DDPM


In this problem we are solving an SDE of the form
$$dx_t = f(x,t)dt + g(t)dw $$
$$dx_t = [f(x,t) - g^2(t)\nabla_{x_t}\log p_t(x_t)]dt + g(t)d\bar{w}$$

We can have any function f and g and solve the SDE in the forward and then sample in the reverse direction.

Generally, we dont have the closed form for $\nabla_{x_t}\log p_t(x_t)$, In this example we have because we know the distribution of $x_0$, which is mixture of gaussian and the also $x_t = x_0  \ast \mathcal{N}(\mu,\sigma)$. However, we wil assume general case in which the distribution of $x_0$ is unknown, for such cases  $\nabla_{x_t}\log p_t(x_t)$ is not tractable.

One advantage we have is $p(x_t|x_0)$ is tractable.

$$\log p(x_t) = \log \mathop{\mathbb{E}}_{x_0} p(x_t|x_0)*p(x_0)$$
$$\log p(x_t) \ge \mathop{\mathbb{E}}_{x_0} [\log p(x_t|x_0)*p(x_0)]$$
$$\nabla_{x_t} \log p(x_t) \ge  \nabla_{x_t} \mathop{\mathbb{E}}_{x_0} [\log p(x_t|x_0)+ \log p(x_0)]$$
$$\nabla_{x_t} \log p(x_t) \ge\mathop{\mathbb{E}}_{x_0} [  \nabla_{x_t} \log p(x_t|x_0)]$$

Based on ELBO property we can replace the probabilty with the conditional probability and an expectation. 

Now we know the closed form of the conditional in forward pass and in the reverse direction we replace $\nabla_{x_t}\log p_t(x_t)$ with $s_{\theta}(x,t)$.

During training we train with a score matching loss
$$L = \mathop{\mathbb{E}}_{x_0}\mathop{\mathbb{E}}_{t \sim U[0,T]} [\lambda(t)||s_{\theta}(x,t) -\nabla_{x_t}\log p_t(x_t)||_2^2 ]$$

Where can Improvements come from? 

There are three imprortant parameters in the score based models the improvements can come

1. Choice of SDE
2. Choice of $\lambda(t)$
3. Choice of model for $s_{\theta}(x,t)$


### Choice of SDE

In the paper DDPM, Ho et.al,  has shown that score models and denoising diffusion models are the same but with a different form for SDE. 

In this we will only write the SDE and the forward process, the backward for the following is avaliable in the code.

In FwdOrnstein-Uhlenbeck Process the forward SDE is given by 
$$f(x_t,t) = -x_t, g(t) = \sqrt{2}$$
$$x_t = e^{-t}x_0 + \sqrt{1- e^{-2t}} \epsilon$$

It can be shown the DDPM is shown as 

$$f(x_t,t) = \frac{-\beta_t}{2}, g(t) = \sqrt{\beta_t}$$
$$x_t =  \sqrt{1- \beta_t}x_0 + \sqrt{\beta_t} \epsilon$$

We can try multiple different $f(x_t,t),g(t)$ However, I donot have background in solving SDE. So  Here we show that using DDPM will improve the KLD metric.



### References
1. Chan, S. H. (2024). Tutorial on Diffusion Models for Imaging and Vision. arXiv preprint arXiv:2403.18103. https://arxiv.org/pdf/2403.18103.pdf.
2. Weng, Lilian. (Jul 2021). What are diffusion models? Lilâ€™Log. https://lilianweng.github.io/posts/2021-07-11-diffusion-models/.
3. Vishnu Boddeti. (2024). Deep Learning. https://hal.cse.msu.edu/teaching/2024-spring-deep-learning/
4. Arash Vahdat. et al. (2022). CVPR. https://cvpr2022-tutorial-diffusion-models.github.io/