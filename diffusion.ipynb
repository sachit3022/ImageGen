{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import skimage as ski\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture of Gaussians\n",
    "\n",
    "We will use a 2D Gaussian mixture model as the data distribution we want to learn. The mixture model is defined as,\n",
    "$$\\rho(x) = \\sum_{k=1}^K \\alpha_k\\mathcal{N}(\\mu_k,\\Sigma_k)$$\n",
    "\n",
    "where each Gaussian in the mixture is parameterized by $(\\alpha,\\mu_x, \\mu_y,\\sigma_x,\\sigma_y)$ i.e., $\\Sigma$ is a diagonal matrix with variance $\\sigma_x^2$ and $\\sigma_y^2$ along the $x$ and $y$ axes, respectively. This allows us to have a closed-form expression for the flows simulated below.\n",
    "\n",
    "The Gaussian mixture and its sampling function have been implemented for you below. We will consider a special case where the each Gaussian in the mixture lies on a circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixture:\n",
    "    def __init__(self, k=3, dim=2):\n",
    "        self.k = k\n",
    "        self.dim = 2\n",
    "        \n",
    "        radius = 10\n",
    "        thetas = torch.arange(k) * 2 * np.pi / k\n",
    "        self.mus = radius * torch.hstack([torch.cos(thetas)[:, None], torch.sin(thetas)[:, None]])\n",
    "        self.sigmas = torch.stack([2 * torch.rand(self.dim) for _ in range(k)])\n",
    "        \n",
    "        temp = torch.rand(k)\n",
    "        self.alphas = temp / temp.sum()\n",
    "\n",
    "    def inv_sigmoid(self, value):\n",
    "        return torch.log(value/(1-value))\n",
    "    \n",
    "    def sample(self, n):\n",
    "        samples = torch.zeros((n, self.dim))\n",
    "        for i in range(n):\n",
    "            # sample uniform\n",
    "            r = torch.rand(1).item()\n",
    "            # select gaussian\n",
    "            k = 0\n",
    "            for j, threshold in enumerate(self.alphas.cumsum(dim=0).tolist()):\n",
    "                if r < threshold:\n",
    "                    k = j\n",
    "                    break\n",
    "\n",
    "            selected_mu = self.mus[k]\n",
    "            selected_cov = self.sigmas[k] * torch.eye(2)\n",
    "\n",
    "            # sample from selected gaussian\n",
    "            lambda_, gamma_ = torch.linalg.eig(selected_cov)\n",
    "            lambda_ = lambda_.real\n",
    "            gamma_ = gamma_.real\n",
    "\n",
    "            dimensions = len(lambda_)\n",
    "            # sampling from normal distribution\n",
    "            y_s = torch.rand((dimensions * 1, 3))\n",
    "            x_normal = torch.mean(self.inv_sigmoid(y_s), axis=1).reshape((-1, dimensions))\n",
    "            # transforming into multivariate distribution\n",
    "            samples[i] = (x_normal * lambda_) @ gamma_ + selected_mu\n",
    "            \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample some data and plot the density\n",
    "np.random.seed(0)\n",
    "gmm = GaussianMixture(k=8)\n",
    "samples = gmm.sample(1000)\n",
    "plot_density_from_samples(samples, filepath='plots/1-samples.png', show=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now implement the density function $\\rho(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density function for a single gaussian distribution given its parameters\n",
    "def get_gaussian_likelihood(x, mu, sigma, dim=2):\n",
    "    # ------------\n",
    "    # FILL THIS IN - START\n",
    "    # ------------\n",
    "    \n",
    "    # compute the likelihood of a single gaussian distribution\n",
    "    \n",
    "    # ------------\n",
    "    # FILL THIS IN - END\n",
    "    # ------------\n",
    "    pass\n",
    "\n",
    "# density function for the mixture of gaussians\n",
    "def rho0(samples, alphas, mus, sigmas):\n",
    "    likelihood = 0\n",
    "    for i in range(len(mus)):\n",
    "        likelihood += alphas[i] * get_gaussian_likelihood(samples, mus[i], sigmas[i])\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us sample visualize the density $\\rho(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 2D grid of points\n",
    "x = torch.linspace(-15, 15, 250)\n",
    "y = torch.linspace(-15, 15, 250)\n",
    "grid_x, grid_y = torch.meshgrid(x, y, indexing='ij')\n",
    "grid_samples = torch.stack([grid_x, grid_y], dim=-1).view(-1, 2)\n",
    "\n",
    "# plot the density\n",
    "rho = rho0(grid_samples, gmm.alphas, gmm.mus, gmm.sigmas)\n",
    "plot_density(grid_samples, rho, filepath='plots/2-density.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langevin Sampling\n",
    "\n",
    "To sample from a density $\\rho(x)$, Langevin diffusion performs a gradient descent on the negative log-density as follows,\n",
    "$$x_{k+1} := x_k + \\tau \\eta(x_k) + \\sqrt{2\\tau}w_k$$\n",
    "\n",
    "where $\\tau>0$ is the step size and $w_k \\sim \\mathcal{N}(0,I_d)$ is a standard Gaussian white noise. The score function $\\eta(x)$ is defined as, \n",
    "$$\\eta(x) := \\nabla \\log \\rho(x) = \\frac{\\nabla \\rho(x)}{\\rho(x)}.$$\n",
    "\n",
    "When $\\tau \\rightarrow 0$, this corresponds to the following Langevin stochastic differential equation (SDE),\n",
    "$$dx_t = \\eta(x_t)dt + \\sqrt{2}dw_t$$\n",
    "where $w_t$ is the Weiner process. We will first compute and display the score as a vector field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score function $\\eta(x)$ can be estimated from samples if you have access to the density $\\rho(x)$ as follows. We already implemented the score function for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta0(samples, alphas, mus, sigmas):\n",
    "    x = samples.detach().clone()\n",
    "    x.requires_grad = True\n",
    "    p_x = rho0(x, alphas, mus, sigmas)\n",
    "    \n",
    "    loss1 = torch.sum(p_x.log())\n",
    "    loss1.backward()\n",
    "    output = x.grad\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 2D grid of points\n",
    "x = torch.linspace(-15, 15, 30)\n",
    "y = torch.linspace(-15, 15, 30)\n",
    "grid_x, grid_y = torch.meshgrid(x, y, indexing='ij')\n",
    "grid_samples = torch.stack([grid_x, grid_y], dim=-1).view(-1, 2)\n",
    "\n",
    "# plot the score function\n",
    "eta = eta0(grid_samples, gmm.alphas, gmm.mus, gmm.sigmas)\n",
    "plot_score_function(grid_samples, eta, filepath='plots/3-score_function.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, Langevin diffusion performs a gradient descent on the negative log-density as follows,\n",
    "$$x_{k+1} := x_k + \\tau \\eta(x_k) + \\sqrt{2\\tau}w_k$$\n",
    "\n",
    "where $\\tau>0$ is the step size and $w_k \\sim \\mathcal{N}(0,I_d)$ is a standard Gaussian white noise.\n",
    "\n",
    "You will now implement the Langevin sampling algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us run the Langevin sampling and display the density as it evolves.\n",
    "\n",
    "P = 10000 # number of particles\n",
    "w = 15 # support of the space\n",
    "z0 = 2 * w * torch.rand((P, 2)) - w\n",
    "\n",
    "T = 10 # final time\n",
    "N = 25 # number of steps\n",
    "tau = T / N # step size\n",
    "z = torch.zeros((P, 2, N))\n",
    "z[:,:, 0] = z0\n",
    "for i in tqdm.tqdm(range(N-1)):\n",
    "    # ------------\n",
    "    # FILL THIS IN - START\n",
    "    # ------------\n",
    "    \n",
    "    eta = ...\n",
    "    z[:,:,i+1] = ...\n",
    "    \n",
    "    # ------------\n",
    "    # FILL THIS IN - END\n",
    "    # ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gif of the density as it evolves\n",
    "\n",
    "import imageio.v3 as iio\n",
    "from pygifsicle import optimize\n",
    "\n",
    "gif_path = 'plots/4-density-from-samples-evolution-ref-to-data-langevin.gif'\n",
    "frames = []\n",
    "for i in tqdm.tqdm(range(N)):\n",
    "    fname = f'plots/density-{i}.jpeg'\n",
    "    plot_density_from_samples(z[:,:,i], show=False, save=True, filepath=fname)\n",
    "    frames.append(ski.io.imread(fname))\n",
    "    os.remove(fname)\n",
    "    \n",
    "frames = np.stack(frames)\n",
    "iio.imwrite(gif_path,frames)\n",
    "optimize(gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will display the generated animation of the samples.\n",
    "\n",
    "![density from samples evolution ref to data langevin](./plots/density-from-samples-evolution-ref-to-data-langevin.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while Langevin can be used for sampling from the density, it needs access to the score function $\\eta(x)$ which is not always available. And, in practice Langevin is slow to converge. So we will look at a different method to sample from the density, but which only needs access to the smoothed score $\\eta_t(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Ulenbeck-Ornstein Process\n",
    "\n",
    "We will define the forward Ulenbeck-Ornstein process here. It is defined as,\n",
    "$$dx_t = -x_t dt + \\sqrt{2}dw_t$$\n",
    "where $w_t$ is a Wiener process.\n",
    "\n",
    "We can numerically approximate the solution of the above SDE using the Euler-Maruyama method as follows,\n",
    "$$x_{k+1} = x_k - \\tau x_k + \\sqrt{2\\tau}w_t$$\n",
    "\n",
    "Given $x_0$ and $t$ we can sample $x_t$ using the closed-form solution of the above Ulenbeck-Ornstein process.\n",
    "$$x_t = e^{-t} x_0 + \\sqrt{(1-e^{-2t})}z$$\n",
    "with $z\\sim \\mathcal{N}(0,I_d)$.\n",
    "\n",
    "In the case of the Gaussian mixture that we considered,\n",
    "\n",
    "$$x_0 \\sim \\rho_0(x) \\stackrel{\\Delta}{=} \\sum_{k=1}^K \\alpha_k\\mathcal{N}(\\mu_k,\\Sigma_k)$$\n",
    "\n",
    "Now analytically compute the density $\\rho_t(x)$ at time $t$.\n",
    "$$x_t \\sim \\rho_t(x) \\stackrel{\\Delta}{=} ...$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will now implement the analytical density at time t using the mean and covariance of the GMM at time zero.\n",
    "def rhot(x, t, alphas, mus, sigmas):\n",
    "    # ------------\n",
    "    # FILL THIS IN - START\n",
    "    # ------------\n",
    "    \n",
    "    pass\n",
    "    \n",
    "    # ------------\n",
    "    # FILL THIS IN - END\n",
    "    # ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us visualize the density evolution\n",
    "\n",
    "# create a 2D grid of points\n",
    "x = torch.linspace(-15, 15, 100)\n",
    "y = torch.linspace(-15, 15, 100)\n",
    "grid_x, grid_y = torch.meshgrid(x, y, indexing='ij')\n",
    "grid_samples = torch.stack([grid_x, grid_y], dim=-1).view(-1, 2)\n",
    "\n",
    "frames = []\n",
    "t_list = torch.arange(0, 10, 0.1)\n",
    "for i in tqdm.tqdm(range(len(t_list))):\n",
    "    rho_t = rhot(grid_samples, t_list[i], gmm.alphas, gmm.mus, gmm.sigmas)\n",
    "    fname = f'plots/density-t-{i}.png'\n",
    "    plot_density(grid_samples, rho_t, show=False, save=True, filepath=fname)\n",
    "    frames.append(ski.io.imread(fname))\n",
    "    os.remove(fname)\n",
    "\n",
    "gif_path = 'plots/5-density-evolution-data-to-ref-ou.gif'\n",
    "frames = np.stack(frames)\n",
    "iio.imwrite(gif_path,frames)\n",
    "optimize(gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will display the generated animation of the density.\n",
    "\n",
    "![density evolution data to ref ou](./plots/density-evolution-data-to-ref-ou.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the forward Ulenbeck-Ornstein process is defined as,\n",
    "$$dx_t = -x_t dt + \\sqrt{2}dw_t$$\n",
    "where $w_t$ is a Wiener process.\n",
    "\n",
    "We can numerically approximate the solution of the above SDE using the Euler-Maruyama method as follows,\n",
    "$$x_{k+1} = x_k - \\tau x_k + \\sqrt{2\\tau}w_t$$\n",
    "\n",
    "Given $x_0$ and $t$ we can sample $x_t$ using the closed-form solution of the above Ulenbeck-Ornstein process.\n",
    "$$x_t = e^{-t} x_0 + \\sqrt{(1-e^{-2t})}z$$\n",
    "with $z\\sim \\mathcal{N}(0,I_d)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will now implement the forward Ornstein-Ulhenbeck process.\n",
    "\n",
    "class FwdOrnsteinUhlenbeckProcess:\n",
    "    def __init__(self, tspan=(0., 1.)):\n",
    "        self.tspan = tspan\n",
    "        self.N = 2500\n",
    "\n",
    "    def f(self, u, p, t):\n",
    "        return -u\n",
    "\n",
    "    def g(self, u, p, t):\n",
    "        return np.sqrt(2) * np.ones(u.shape)\n",
    "\n",
    "    def solve(self, u0, t, type='exact'):\n",
    "        if type == 'exact':\n",
    "            out = 0\n",
    "            pass\n",
    "        if type == 'sde':\n",
    "            out = 0\n",
    "            pass\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let us check if the SDE is working correctly\n",
    "\n",
    "# initilize the process\n",
    "noising_process = FwdOrnsteinUhlenbeckProcess()\n",
    "\n",
    "# get data\n",
    "cat = ski.data.chelsea()\n",
    "img_size = (int(cat.shape[0]*0.1), int(cat.shape[1]*0.1), 3)\n",
    "cat = ski.transform.resize(cat, img_size) - 0.5\n",
    "\n",
    "# call the SDE\n",
    "u0 = torch.from_numpy(cat.flatten()).float()\n",
    "out = noising_process.solve(u0, 0.1, type='sde')\n",
    "out = np.reshape(out, cat.shape, order='C').numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(cat + 0.5)\n",
    "img = np.divide(out - np.min(out), np.max(out) - np.min(out))\n",
    "ax[1].imshow(img)\n",
    "ax[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we checked the evolution of the denisty $\\rho_t(x)$ for different values of $t$. Now let us evolve samples through the Ulenbeck-Ornstein process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us get some samples and show the evolution of the density estimated from the samples\n",
    "\n",
    "P = 1000 # number of particles\n",
    "samples = gmm.sample(P)\n",
    "\n",
    "# initialize the foward process\n",
    "noising_process = FwdOrnsteinUhlenbeckProcess(tspan=(0., 10.))\n",
    "\n",
    "frames = []\n",
    "t_list = torch.arange(0, 10, 0.1)\n",
    "for i in tqdm.tqdm(range(len(t_list))):\n",
    "    rho_t = noising_process.solve(samples, t_list[i].item(), type='sde')\n",
    "    fname = f'plots/density-t-{i}.png'\n",
    "    plot_density_from_samples(rho_t, show=False, save=True, filepath=fname)\n",
    "    frames.append(ski.io.imread(fname))\n",
    "    os.remove(fname)\n",
    "\n",
    "gif_path = 'plots/6-density-from-samples-evolution-data-to-ref-ou.gif'\n",
    "frames = np.stack(frames)\n",
    "iio.imwrite(gif_path,frames)\n",
    "optimize(gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the evolution of the samples through the Ulenbeck-Ornstein process.\n",
    "\n",
    "![density from samples evolution data to ref ou](./plots/density-from-samples-evolution-data-to-ref-ou.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothed Score Function\n",
    "\n",
    "The reverse Ulnebeck-Ornstein process requires us to compute the score function at any time $t$.\n",
    "$$\\eta_t(x) := \\nabla \\log(\\rho_t(x)) = \\frac{\\nabla \\rho_t(x)}{\\rho_t(x)}$$\n",
    "\n",
    "Informally, the score points in the direction of regions with high densities.\n",
    "\n",
    "Since the density $\\rho_t(x)$ for a Gaussian mixture can be computed in closed-form, the score function $\\eta_t(x)$ can also be computed in closed-form. However, in a real-scenario, we will not know the density $\\rho_t(x)$ and only have access to the samples. So in this case, the score function has to be estimated from the samples.\n",
    "\n",
    "We will see how to estimate the score function from samples soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will now implement the new score function at time t\n",
    "def etat(x, t, alphas, mus, sigmas):\n",
    "    # ------------\n",
    "    # FILL THIS IN - START\n",
    "    # ------------\n",
    "    \n",
    "    pass\n",
    "    \n",
    "    # ------------\n",
    "    # FILL THIS IN - END\n",
    "    # ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us now visualize the score function evolution\n",
    "\n",
    "# create a 2D grid of points\n",
    "x = torch.linspace(-15, 15, 30)\n",
    "y = torch.linspace(-15, 15, 30)\n",
    "grid_x, grid_y = torch.meshgrid(x, y, indexing='ij')\n",
    "grid_samples = torch.stack([grid_x, grid_y], dim=-1).view(-1, 2)\n",
    "\n",
    "frames = []\n",
    "t_list = torch.arange(0, 10, 0.1)\n",
    "for i in tqdm.tqdm(range(len(t_list))):\n",
    "    eta_t = etat(grid_samples, t_list[i], gmm.alphas, gmm.mus, gmm.sigmas)\n",
    "    fname = f'plots/score-function-t-{i}.png'\n",
    "    plot_score_function(grid_samples, eta_t, show=False, save=True, filepath=fname)\n",
    "    frames.append(ski.io.imread(fname))\n",
    "    os.remove(fname)\n",
    "\n",
    "gif_path = 'plots/7-score-function-evolution.gif'\n",
    "frames = np.stack(frames)\n",
    "iio.imwrite(gif_path,frames)\n",
    "optimize(gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse Ulenbeck-Ornstein Process\n",
    "\n",
    "We will define the reverse Ulenbeck-Ornstein process here. It is defined as,\n",
    "$$dy_t = {\\left[y_t + (1+\\beta)\\nabla\\log \\rho_{T-t}(y_t)\\right]}dt + \\sqrt{2\\beta}dw_t$$\n",
    "where $w_t$ is a Wiener process, and $\\beta \\geq 0$ is a parameter controlling the randomness introduced during the reverse process. In standard diffusion models $\\beta=1$. And, when $\\beta=0$, the reverse process is probability flow ODE sampler we saw in class.\n",
    "\n",
    "This reverse process can be discretize using the Euler-Maruyama scheme as follows,\n",
    "$$y_{k+1} = y_{k} + \\tau(y_k + (1+\\beta)\\nabla\\log(\\rho_{T-t}(y_k))) + \\sqrt{2\\tau\\beta}w_k$$\n",
    "\n",
    "Here $y_0 \\sim \\mathcal{N}(0, I_d)$.\n",
    "\n",
    "Note that here, unlike the Langevin sampling, we only need access to the smooth score $\\eta_t(x)$, rather than the true score $\\eta(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us implement the reverse Euler-Maruyama scheme.\n",
    "\n",
    "P = 1000 # number of particles\n",
    "y0 = torch.randn((P, 2))\n",
    "\n",
    "N = 2500 # number of steps\n",
    "T = 5 # final time\n",
    "tau = T / N # step size\n",
    "\n",
    "t = torch.Tensor([T])\n",
    "beta = 0.1\n",
    "\n",
    "const_temp1 = torch.Tensor([(1+beta)])\n",
    "const_temp2 = torch.Tensor([tau * beta])\n",
    "\n",
    "y = torch.zeros((P, 2, N))\n",
    "for i in range(N-1):\n",
    "    # ------------\n",
    "    # FILL THIS IN - START\n",
    "    # ------------\n",
    "    \n",
    "    eta_t = ...\n",
    "    y[:,:,i+1] = ...\n",
    "    t = ...\n",
    "    \n",
    "    # ------------\n",
    "    # FILL THIS IN - END\n",
    "    # ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us visualize the reverse density evolution\n",
    "\n",
    "frames = []\n",
    "for i in tqdm.tqdm(range(0, N-1, 100)):\n",
    "    plot_density_from_samples(y[:,:,i], filepath=f'plots/reverse-density-{i}.png', show=False, save=True)\n",
    "    img = ski.io.imread(f'plots/reverse-density-{i}.png')\n",
    "    frames.append(img)\n",
    "    os.remove(f'plots/reverse-density-{i}.png')\n",
    "\n",
    "gif_path = 'plots/8-density-from-samples-evolution-ref-to-data-ou.gif'\n",
    "frames = np.stack(frames)\n",
    "iio.imwrite(gif_path,frames)\n",
    "optimize(gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the evolution of the density in reverse.\n",
    "\n",
    "![density from samples evolution ref to data ou](./plots/density-from-samples-evolution-ref-to-data-ou.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising Score Matching\n",
    "\n",
    "Running the reverse Ulenbeck-Ornstein process requires access to the smoothed score $\\eta_t(x)=\\nabla \\log \\rho_t(x)$. In practice, we will not have access to the true score function $\\eta_t(x)$ or the density $\\rho_t(x)$, but only have access to the samples.\n",
    "\n",
    "So the main idea of Score Matching is to approximate this score using a function computed from the samples $X_t$.\n",
    "$$\\nabla \\log \\rho_t(x) \\approx \\phi_{\\theta}(x, t)$$\n",
    "where $\\phi_{\\theta}(x, t)$ is a neural network with parameters $\\theta$.\n",
    "\n",
    "We will optimize the parameters $\\theta$ to minimize the following loss function,\n",
    "$$\\min_{\\theta} \\mathbb{E}_{t}\\left(\\lambda_t\\mathbb{E}_{x_t}\\|\\nabla \\log \\rho_t(x_t) - \\phi_{\\theta}(x_t, t)\\|^2\\right)$$\n",
    "where $\\lambda_t$ is a weighting factor that can be used to control the importance of the loss at different times $t$.\n",
    "\n",
    "From the lecture we saw that $\\nabla_{x_t}\\log \\rho_t(x_t) = \\mathbb{E}[\\nabla_{x_t}\\log \\rho_{t|0}(x_t|x_0)|x_t]$, i.e., the score function at time $t$ can be computed as the conditional expectation of the score function of the conditional density $\\rho_{t|0}(x_t|x_0)$.\n",
    "\n",
    "Since we know the forward noising process i.e., $X_t = e^{-t}X_0 + \\sqrt{1-e^{-2t}}Z$, we can compute the conditional density in closed-form as $\\rho_{t|0}(X_t|X_0) \\stackrel{\\Delta}{=} \\mathcal{N}(X_t;e^{-t}X_0, 1-e^{-2t})$. \n",
    "\n",
    "Derive the score function of the conditional distribution,\n",
    "\n",
    "$$\\nabla_{x_t}\\log \\rho_{t|0}(x_t|x_0) = ...$$\n",
    "\n",
    "So the overall objective for optimizing the neural network parameters is,\n",
    "$$\\min_{\\theta} \\mathbb{E}_{t}\\left(\\lambda_t\\mathbb{E}_{(x_0, z) \\sim (p(X_0), p(X_t)) }\\left\\|\\nabla_{x_t}\\log \\rho_{t|0}(x_t|x_0) - \\phi_{\\theta}(z, t)\\right\\|^2\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First generate some samples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20000\n",
    "x0 = gmm.sample(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define uniform time distribution between $[10^{-2}, T]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tmin = 1e-2\n",
    "Tmax = T\n",
    "t_list = torch.linspace(Tmin, Tmax, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create $X_t$ from $X_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn((N, 2))\n",
    "temp = t_list.repeat(2,1).T\n",
    "xt = torch.exp(-temp) * x0 + torch.sqrt(1 - torch.exp(-2*temp)) * w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map $t$ into a sane range for ease of training and stack it with $X_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_nn = (t_list - t_list.min()) / (t_list.max() - t_list.min())\n",
    "t_nn = 2 * t_nn - 1\n",
    "u = torch.hstack([xt, t_nn[:,None]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the ground truth output for the neural network i.e., the score function $\\nabla_{x_t}\\log \\rho_{t|0}(x_t|x_0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------\n",
    "# FILL THIS IN\n",
    "# ------------\n",
    "\n",
    "y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the neural network as a multi-layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "            \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = nn.ModuleList()\n",
    "        layers.append(nn.Linear(d_in, d_hidden[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "        for i in range(1, len(d_hidden)):\n",
    "            layers.append(nn.Linear(d_hidden[i-1], d_hidden[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.Linear(d_hidden[-1], d_out))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(weights_init)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = 3\n",
    "d_hidden = [128, 128, 128]\n",
    "d_out = 2\n",
    "model = MLP(d_in, d_hidden, d_out)\n",
    "loss = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the network in full batch mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 1000\n",
    "loss_list = torch.zeros(num_epochs)\n",
    "\n",
    "for i in tqdm.tqdm(range(num_epochs)):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(u)\n",
    "    l = loss(y_pred, y)\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    loss_list[i] = l.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot(x=range(num_epochs), y=loss_list)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('plots/9-loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the learned model to sample run the reverse Ulenbeck-Ornstein process and sample data points from the distribution.\n",
    "\n",
    "Recall that, the reverse process can be discretize using the Euler-Maruyama scheme as follows,\n",
    "$$y_{k+1} = y_{k} + \\tau(y_k + (1+\\beta)\\nabla\\log(\\rho_{T-t}(y_k))) + \\sqrt{2\\tau\\beta}w_k$$\n",
    "\n",
    "Here $y_0 \\sim \\mathcal{N}(0, I_d)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us implement the reverse Euler-Maruyama scheme.\n",
    "\n",
    "P = 1000 # number of particles\n",
    "y0 = torch.randn((P, 2), requires_grad=False)\n",
    "\n",
    "N = 2500 # number of steps\n",
    "T = 5 # final time\n",
    "tau = T / N # step size\n",
    "\n",
    "t = torch.Tensor([T])\n",
    "beta = 0.1\n",
    "\n",
    "const_temp1 = torch.Tensor([1+beta])\n",
    "const_temp2 = torch.Tensor([tau * beta])\n",
    "\n",
    "y = torch.zeros((P, 2, N), requires_grad=False)\n",
    "for i in tqdm.tqdm(range(N-1)):\n",
    "    t_temp = 2 * ((t.repeat(P, 1)) / (T)) - 1\n",
    "    u = torch.hstack([y[:,:,i], t_temp])\n",
    "    eta_t = model(u)\n",
    "    # ------------\n",
    "    # FILL THIS IN - START\n",
    "    # ------------\n",
    "    y[:,:,i+1] = ...\n",
    "    t = ...\n",
    "    # ------------\n",
    "    # FILL THIS IN - END\n",
    "    # ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us visualize the reverse density evolution\n",
    "\n",
    "y = y.detach().numpy()\n",
    "\n",
    "frames = []\n",
    "for i in tqdm.tqdm(range(0, N-1, 100)):\n",
    "    plot_density_from_samples(y[:,:,i], filepath=f'plots/reverse-density-{i}.png', show=False, save=True)\n",
    "    img = ski.io.imread(f'plots/reverse-density-{i}.png')\n",
    "    frames.append(img)\n",
    "    os.remove(f'plots/reverse-density-{i}.png')\n",
    "\n",
    "gif_path = 'plots/10-density-from-samples-evolution-ref-to-data-nn.gif'\n",
    "frames = np.stack(frames)\n",
    "iio.imwrite(gif_path,frames)\n",
    "optimize(gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the particles sampled using the learned score function.\n",
    "\n",
    "![density from samples evolution ref to data nn](./plots/density-from-samples-evolution-ref-to-data-nn.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus (1 pt): Feel to modify the above algorithm to help improve the efficiency of the sampling/data generation process. You can either modify the training process of the model, or simply improving the sampling process for a standard trained model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
